<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>NLPContributionGraph -- Structuring Scholarly NLP Contributions in the Open Research Knowledge Graph</title>

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="css/font.css" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic,700italic" rel="stylesheet"
          type="text/css">

    <!-- Custom styles for this template -->
    <link href="css/landing-page.css" rel="stylesheet">
	
	<!-- Favicon -->
	<link rel="icon" type="image/ico" href="/img/favicon.ico" />		
	
    <style type="text/css">
        /*body {*/
        /*min-width: 320px;*/
        /*}*/

        /*form {*/
        /*text-align: center;*/
        /*}*/

        /*.hidden {*/
        /*display: none;*/
        /*}*/

        /*.container {*/
        /*margin-bottom: 12px;*/
        /*}*/
        .node circle {
            fill: #fff;
            stroke: rgba(100, 100, 100, 1);
            stroke-width: 1px;
        }

        .node text {
            font: 14px sans-serif;
        }

        .code-txt {
            display: block;
            padding: 8px 24px;
            word-wrap: break-word;
            border-radius: 4px;
            word-break: normal;
            font-weight: 400;
            background-color: #f0f0f0;
            line-height: 1.5em;
            font-size: 1em;
            border: 1px solid #062c33;
            color: #555;
            text-align: left;
            overflow-y: scroll;
            min-height: 40px;
            max-height: 600px;
            margin-bottom: 10px;
        }

        .instructions-legal-information {
            margin: 1.5em auto 0 auto;
            font-size: 0.8em;
            text-align: justify;
        }

        .panel {
            margin-bottom: 16px;
        }

        .panel-instructions {
            border-top-right-radius: 0;
            border-top-left-radius: 0;
        }

        .panel-heading-instructions {
            border-top-right-radius: 0;
            border-top-left-radius: 0;
            cursor: pointer;
        }

        .example-annotation {
            background-color: rgba(25, 148, 213, 0.3);
        }

        .font-size-larger {
            font-size: 18px;
        }

        .row-margin {
            margin-bottom: 20px;
            margin-top: 50px;
        }

        .random-answer-text {
            font-weight: bold;
        }

        .fieldset-readonly {
            color: #777;
        }

        .fieldset-readonly input[type="radio"]:checked, .fieldset-readonly input[type="radio"]:checked + label {
            opacity: 0.5
        }

        .input-follow-up-q {
            min-width: 560px;
        }

        .validation-label {
            font-weight: bold;
            margin-bottom: 4px;
        }

        .validation-initial, .validation-previous {
            display: none;
        }
		
    </style>
</head>
<!--<script src="vendor/jquery/jquery.min.js"></script>-->
<!--<script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>-->
<script type="text/javascript" src="data/sharc1_train.json"></script>
<script src="https://d3js.org/d3.v3.min.js"></script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src=mmd.min.js></script>

<body>

<!-- Navigation -->

<nav class="navbar navbar-expand-md navbar-light bg-light">
 <!--<nav class="navbar navbar-expand-lg navbar-light bg-light fixed-top">-->
    <div class="container">
        <!--<div class="collapse navbar-collapse" id="navbarResponsive">-->
        <div class="" id="navbarResponsive">
            <ul class="navbar-nav mc-auto">
                <li class="nav-item">
                    <a class="nav-link" href="index.html">NCG Home</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="data.html">Data</a>
                </li>
				<li class="nav-item">
					<a class="nav-link" href="googlegroups.html">Q&A Google Group</a>
				</li>				
                <li class="nav-item">
                    <a class="nav-link" href="leaderboard.html">Leaderboard</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="https://www.orkg.org/orkg/" target="_blank"><img
                            src="img/orkg-logo-menu.png" style="width: 20%; height: 20%" alt=""/>Project Page</a>
                </li>
				<li class="nav-item" align="right">				
					<a class="nav-link" href="https://semeval.github.io/SemEval2021/" target="_blank">Visit SemEval 2021 Home</a>
				</li>	
				
            </ul>
        </div>
    </div>
</nav>

<!-- Header -->
<header class="data-header">

    <div class="container">
        <div class="intro-message">
            <h1 style="color:#FF4500;">NLPContributionGraph</h1>
            <h3 style="color:#696969;">Structuring Scholarly NLP Contributions in the Open Research Knowledge Graph</h3>
            <hr class="intro-divider">
            <ul class="list-inline intro-social-buttons">
                <li class="list-inline-item">
                    <a href="data/trial-data.zip" class="btn btn-info btn-lg">
                        <!--<i class="fa fa-download fa-fw"></i>-->
                        <span class="dataset-name">Trial Data Download</span>
                    </a>
                </li>
            </ul>
            <h6 style="color:#696969;">Dataset distribution under <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0 </a>
            </h6>

        </div>
    </div>
</header>

<!-- Page Content -->
<section class="content-section-a">

    <div class="container">
        <div class="row">
            <div class="col-lg-10 ml-auto">
                <hr class="section-heading-spacer">
                <div class="clearfix"></div>
                <h2 class="section-heading">Dataset</h2>
                <p class="lead">
                    NLPContributionGraph comprises a dataset of scholarly articles in Natural Language Processing annotated for their contributions 
					at sentence-level and with (subject,predicate,object) triple statements within the sentences
					wherein the triples taken together form a knowledge graph about the articles' contribution. 
					
					In order to understand the dataset and the task better, we provide <a href="#data_explain">explanation</a> 
					and <a href="visualisation.html">visualisation</a> of the
                    data.
                </p>


            </div>
            <div class="col-lg-5 mr-auto">

            </div>
        </div>

    </div>
    <!-- /.container -->
</section>

<section class="content-section-b">

    <div class="container">
        <div class="row">
            <div class="col-lg-10 ml-auto">
                <hr class="section-heading-spacer">
                <div class="clearfix"></div>
                <h2 class="section-heading"><a name="data_explain">Understanding the Data</a></h2>

                <p class="lead">
					At the top-level, the annotated sentences and triples from the sentences are organized under eight core information units, 
					viz. <font style="font-variant: small-caps">ResearchProblem</font>, 
					<font style="font-variant: small-caps">Approach</font>, <font style="font-variant: small-caps">Objective</font>, 
					<font style="font-variant: small-caps">ExperimentalSetup</font>, <font style="font-variant: small-caps">Results</font>, <font style="font-variant: small-caps">Tasks</font>, <font style="font-variant: small-caps">Experiments</font>, <font style="font-variant: small-caps">AblationAnalysis</font>, and <font style="font-variant: small-caps">Baselines</font>.
				
                    In this section we define these information units.
                </p>

                <hr/>
				
                <table border="0" class="lead" valign="top">
                    <tr>					
                        <th valign="top"><font style="font-variant: small-caps">ResearchProblem:</font> </th>
                        <td>&nbsp;</td>
                        <td>It determines the research challenge addressed by a contribution using the predicate <i>hasResearchProblem</i>. By definition, it is the focus of the research investigation, in other words, <i>the issue for which the solution must be obtained</i>.</td>
                    </tr>
                    <tr>
                        <th valign="top"><font style="font-variant: small-caps">Approach:</font></th>
                        <td>&nbsp;</td>
                        <td>Depending on the paper's content, is referred to as <font style="font-variant: small-caps">Model</font> or <font style="font-variant: small-caps">Method</font> or 
						<font style="font-variant: small-caps">Architecture</font> or <font style="font-variant: small-caps">System</font> or <font style="font-variant: small-caps">Application</font>. 
						Essentially, this is the contribution of the paper as <i>the solution proposed for the research problem</i>.
                        </td>
                    </tr>
                    <tr>
                        <th valign="top"><font style="font-variant: small-caps">Objective:</font></th>
                        <td>&nbsp;</td>
                        <td>It is the <i>defined function for the machine learning algorithm to optimize over</i>.</td>
                    </tr>
                    <tr>
                        <th valign="top"><font style="font-variant: small-caps">ExperimentalSetup:</font></th>
                        <td>&nbsp;</td>
                        <td>Includes details about the platform including both hardware (e.g., GPU) and software (e.g., Tensorflow library) for implementing the machine learning solution; and of variables, that determine the network structure (e.g., number of hidden units) 
						and how the network is trained (e.g., learning rate), for tuning the software to the task objective. Depending on the paper's content, if the implementation details are not specified, it is called <font style="font-variant: small-caps">Hyperparameters</font> instead. </td>
                    </tr>
                    <tr>
                        <th valign="top"><font style="font-variant: small-caps">Results:</font></th>
                        <td>&nbsp;</td>
                        <td>The main findings or outcomes reported in the article text for the <font style="font-variant: small-caps">ResearchProblem</font>.</td>
                    </tr>
                    <tr>
                        <th valign="top"><font style="font-variant: small-caps">Tasks:</font></th>
                        <td>&nbsp;</td>
                        <td>The <font style="font-variant: small-caps">Approach</font> or <font style="font-variant: small-caps">Model</font>, 
						particularly in multi-task settings, are tested on more than one task, in which case, we list all the experimental tasks. 
						The experimental tasks are often synonymous with the experimental datasets since it is common in NLP for tasks to be defined over datasets. 
						And where lists of <font style="font-variant: small-caps">Tasks</font> are concerned, 
						the <font style="font-variant: small-caps">Tasks</font> can include the <font style="font-variant: small-caps">ExperimentalSetup</font> 
						as a sub information unit.</td>
                    </tr>
                    <tr>
                        <th valign="top"><font style="font-variant: small-caps">Experiments:</font></th>
                        <td>&nbsp;</td>
                        <td>
						It is a container information unit that includes one or more of the previous discussed units as sub information units. 
						Can include a combination of <font style="font-variant: small-caps">ExperimentalSetup</font> and <font style="font-variant: small-caps">Results</font>, 
						or it can be combination of lists of <font style="font-variant: small-caps">Tasks</font> and their <font style="font-variant: small-caps">Results</font>, 
						or a combination of <font style="font-variant: small-caps">Approach</font>, <font style="font-variant: small-caps">ExperimentalSetup</font> and <font style="font-variant: small-caps">Results</font>.
                        </td>
                    </tr>
                    <tr>
                        <th valign="top"><font style="font-variant: small-caps">AblationAnalysis:</font></th>
                        <td>&nbsp;</td>
                        <td>It is a form of <font style="font-variant: small-caps">Results</font> that describes the performance of components in an <font style="font-variant: small-caps">Approach</font> or <font style="font-variant: small-caps">Model</font>.
                        </td>
                    </tr>
                    <tr>
                        <th valign="top"><font style="font-variant: small-caps">Baselines:</font></th>
                        <td>&nbsp;</td>
                        <td>They are the listed systems that a proposed <font style="font-variant: small-caps">Approach</font> or <font style="font-variant: small-caps">Model</font> is compared against. </td>
                    </tr>
                </table>
                <p></p>
                <!-- <p>NOTE: The input to a system should include <i>snippet, question, scenario and history</i> <b>ONLY</b>. <font color="red">Evidence and answer should not be included in the input to a
                    prediction model. </font></p> -->
                <hr/>
                <p class="lead">Below, we provide five examples of different types of information units supported with an explanation of the annotated data.</p>
                <!-- Example 2-->
                <u><h4 class="section-heading">Example #1</h4></u>
                <p class="lead">
					In this example, the <font style="font-variant: small-caps">ResearchProblem</font> information unit is modeled. 
					The reference paper is <a href="https://www.aclweb.org/anthology/N18-1202" target="_blank">Deep contextualized word representations</a>. 
					For this information unit, the subject node is  <font style="font-variant: small-caps">Contribution</font> which is left unspecified. 
					It is then followed by the predicate "<i>has research problem</i>." Following this, the three phrases which were found to represent the research problem
					are annotated. Further, the sentences from where the phrases are taken are also annotated with predicate "<i>from sentence</i>." Note, sentence "TITLE" is a variable corresponding to the title of the paper.

                </p>
                <div class="code-txt">
                <pre>
{
  "has research problem" : [
    ["Deep contextualized word representations", {"from sentence" : "TITLE"}],
    ["new type of deep contextualized word representation", {"from sentence" : "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy)."}],
    ["ELMo (Embeddings from Language Models) representations", {"from sentence" : "For this reason, we call them ELMo (Embeddings from Language Models) representations."}]
    ]
}
				</pre>
                </div>
				
				<p class="lead"> The annotations as triples are: </p>
                <div class="code-txt">
                <pre>
(<font style="font-variant: small-caps">Contribution</font>, has research problem, <font style="font-variant: small-caps">Deep contextualized word representations</font>)
(<font style="font-variant: small-caps">Contribution</font>, has research problem, <font style="font-variant: small-caps">new type of deep contextualized word representation</font>)
(<font style="font-variant: small-caps">Contribution</font>, has research problem, <font style="font-variant: small-caps">ELMo (Embeddings from Language Models) representations</font>)
				</pre>
                </div>				
				
				<p class="lead"> The machine reading task will entail, first, identifying sentences with the research problem, and second, 
				extracting the precise research problem phrase spans from the sentences selected in the first step.</p>
				
                <p></p>
                <!-- Example 2-->
                <u><h4 class="section-heading">Example #2</h4></u>
                <p class="lead">
                    In this example, the <font style="font-variant: small-caps">AblationAnalysis</font> information unit is modeled. 
                    The reference paper is <a href="https://www.aclweb.org/anthology/D18-1244/" target="_blank">Graph Convolution over Pruned Dependency Trees Improves Relation Extraction</a>.
					The <font style="font-variant: small-caps">AblationAnalysis</font> information unit is connected to <font style="font-variant: small-caps">Contribution</font> via the predicate "has".
					Note, unlike the previous example that had a single evidence sentence provided against each annotated object, in this example, 
					the evidence sentences are annotated as a paragraph.
					This is to avoid introducing more nested dictionaries or arrays in JSON format.
					Consider, after the top-level information unit node <font style="font-variant: small-caps">AblationAnalysis</font>,
					the annotations are all nested under entity "TACRED dev set," which can be interpreted as ablation experiment results that were
					performed on this dataset. Owing to this nesting, to avoid complicated JSON format, the evidence sentences 
					for the sentence-based annotations are not singled out on a per-sentence basis
					close to the annotated phrase node, but are written together as a paragraph.
                </p>
                <div class="code-txt">
                <pre>
{
  "has" : {
    "Ablation Analysis" : {
      "on" : {
        "TACRED dev set" : {
          "from" : {
            "entity representations and feedforward layers" : {
              "contribute" : "1.0 F1"    
            }
          },
          "remove" : {
            "dependency structure" : {
              "score drops" : "by 3.2 F1"
            },
            "the feedforward layers, the LSTM component and the dependency structure" : {
              "F1 drops" : "by 10.3"
            }
          },
          "removing" : {
            "pruning (i.e., using full trees as input)" : {
              "hurts the result" : "by another 9.7 F1"
            }
          }
        },
        "from sentence" : "To study the contribution of each component in the C-GCN model, we ran an ablation study on the TACRED dev set (Table 3). We find that: (1) The entity representations and feedforward layers contribute 1.0 F1. (2) When we remove the dependency structure (i.e., setting A to I), the score drops by 3.2 F1. (3) F1 drops by 10.3 when we remove the feedforward layers, the LSTM component and the dependency structure altogether. (4) Removing the pruning (i.e., using full trees as input) further hurts the result by another 9.7 F1."
      }
    }
  }
}
				</pre>
                </div>
				
				<p class="lead"> From the above annotations, the following ten triples are obtained. These annotations comprise three levels of nested data overall.</p>
                <div class="code-txt">
                <pre>
(<font style="font-variant: small-caps">Contribution</font>, has, <font style="font-variant: small-caps">Ablation Analysis</font>)
&nbsp;&nbsp;(<font style="font-variant: small-caps">Ablation Analysis</font>, on, <font style="font-variant: small-caps">TACRED dev set</font>)
&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">TACRED dev set</font>, from, <font style="font-variant: small-caps">entity representations and feedforward layers</font>)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">entity representations and feedforward layers</font>, contribute, <font style="font-variant: small-caps">1.0 F1</font>)
&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">TACRED dev set</font>, remove, <font style="font-variant: small-caps">dependency structure</font>)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">dependency structure</font>, score drops, <font style="font-variant: small-caps">by 3.2 F1</font>)
&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">TACRED dev set</font>, remove, <font style="font-variant: small-caps">the feedforward layers, the LSTM component and the dependency structure</font>)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">the feedforward layers, the LSTM component and the dependency structure</font>, F1 drops, <font style="font-variant: small-caps">by 10.3</font>)
&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">TACRED dev set</font>, removing, <font style="font-variant: small-caps">pruning (i.e., using full trees as input)</font>)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">pruning (i.e., using full trees as input)</font>,hurts the result, <font style="font-variant: small-caps">by another 9.7 F1</font>)
				</pre>
                </div>				
				
                <p></p>								
                <!-- Example 3-->
                <u><h4 class="section-heading">Example #3</h4></u>
                <p class="lead">
                    In this example, the <font style="font-variant: small-caps">Results</font> information unit is modeled. 
					The reference paper is <a href="https://www.aclweb.org/anthology/W18-5406/" target="_blank">On the Role of Text Preprocessing in Neural Network Architectures: An Evaluation Study on Text Categorization and Sentiment Analysis</a>.					
                </p>
                <div class="code-txt">
                <pre>
{
  "has" : {
    "Results" : {
      "from" : {
        "Experiment 1: Preprocessing effect" : {
          "observe" : {
            "a certain variability of results" : {
              "depending on" : "preprocessing techniques",
              "average variability" : {
                "+-2.4%" : {
                  "for" : "CNN+LSTM model"
                }
              }
            },
            "from sentence" : "We observe a certain variability of results depending on the preprocessing techniques used (average variability of ±2.4% for the CNN+LSTM model, including a statistical significance gap in seven of the nine datasets), which proves the influence of preprocessing on the final results."
          },
          "does not help" : {
            "more complex pre-processing techniques" : {
              "such as" : "lemmatization and multiword grouping"
            },
            "from sentence" : "Nevertheless, the use of more complex preprocessing techniques such as lemmatization and multiword grouping does not help in general."
          }
        },
        "Experiment 2: Cross-preprocessing" : {
          "exhibiting a better performance" : {
            "multiword enhanced vectors" : {
              "on" : ["single CNN model (best overall performance in seven of the nine datasets)", "CNN+LSTM model (best performance in four datasets and in the same ballpark as the best results in four of the remaining five datasets)"]
            },
            "from sentence" : "In this experiment we observe a different trend, with multiword enhanced vectors exhibiting a better performance both on the single CNN model (best overall performance in seven of the nine datasets) and on the CNN+LSTM model (best performance in four datasets and in the same ballpark as the best results in four of the remaining five datasets)."            
          },
          "consistently better results" : {
            "multiword-wise embeddings" : {
              "on" : {
                "vanilla setting" : {
                  "in" : "eight of the nine datasets"
                }
              }
            },
            "from sentence" : "Interestingly, using multiword-wise embeddings on the vanilla setting leads to consistently better results than using them on the same multiword grouped preprocessed dataset in eight of the nine datasets."
          },
          "competitive" : ["embeddings trained on a simple tokenized corpus", {"from sentence" : "Apart from this somewhat surprising finding, the use of the embeddings trained on a simple tokenized corpus (i.e. vanilla) proved again competitive, as different preprocessing techniques such as lowercasing and lemmatizing do not seem to help"}],
          "do not seem to help" : [{"different preprocessing techniques" : {
            "such as" : "lowercasing and lemmatizing"}}, {"from sentence" : "Apart from this somewhat surprising finding, the use of the embeddings trained on a simple tokenized corpus (i.e. vanilla) proved again competitive, as different preprocessing techniques such as lowercasing and lemmatizing do not seem to help"}]
        }
      }
    }
  }
}
				</pre>
                </div>				
				
				<p class="lead">From the above annotations, the following 18 triples are obtained. These annotations comprise four levels of nested data overall.</p>
                <div class="code-txt">
                <pre>
(<font style="font-variant: small-caps">Contribution</font>, has, <font style="font-variant: small-caps">Results</font>)
&nbsp;&nbsp;(<font style="font-variant: small-caps">Results</font>, from, <font style="font-variant: small-caps">Experiment 1: Preprocessing effect</font>)
&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">Experiment 1: Preprocessing effect</font>, observe, <font style="font-variant: small-caps">a certain variability of results</font>)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">a certain variability of results</font>, depending on, <font style="font-variant: small-caps">preprocessing techniques</font>)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">a certain variability of results</font>, average variability, <font style="font-variant: small-caps">+-2.4%</font>)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">+-2.4%</font>, for, <font style="font-variant: small-caps">CNN+LSTM model</font>)
&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">Experiment 1: Preprocessing effect</font>, does not help, <font style="font-variant: small-caps">more complex pre-processing techniques</font>)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">more complex pre-processing techniques</font>, such as, <font style="font-variant: small-caps">lemmatization and multiword grouping</font>)
&nbsp;&nbsp;(<font style="font-variant: small-caps">Results</font>, from, <font style="font-variant: small-caps">Experiment 2: Cross-preprocessing</font>)
&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">Experiment 2: Cross-preprocessing</font>, exhibiting a better performance, <font style="font-variant: small-caps">multiword enhanced vectors</font>)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">multiword enhanced vectors</font>, on, <font style="font-variant: small-caps">single CNN model (best overall performance in seven of the nine datasets)</font>)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">multiword enhanced vectors</font>, on, <font style="font-variant: small-caps">CNN+LSTM model (best performance in four datasets and in the same ballpark as the best results in four of the remaining five datasets)</font>)
&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">Experiment 2: Cross-preprocessing</font>, consistently better results, <font style="font-variant: small-caps">multiword-wise embeddings</font>)	
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">multiword-wise embeddings</font>, on, <font style="font-variant: small-caps">vanilla setting</font>)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">vanilla setting</font>, in, <font style="font-variant: small-caps">eight of the nine datasets</font>)
&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">Experiment 2: Cross-preprocessing</font>, competitive, <font style="font-variant: small-caps">embeddings trained on a simple tokenized corpus</font>)	
&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">Experiment 2: Cross-preprocessing</font>, do not seem to help, <font style="font-variant: small-caps">different preprocessing techniques</font>)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">different preprocessing techniques</font>, such as, <font style="font-variant: small-caps">lowercasing and lemmatizing</font>)
				</pre>
                </div>							
				
				<p> </p>
                <!-- Example 4-->
                <u><h4 class="section-heading">Example #4</h4></u>
                <p class="lead">
                    In this example, the <font style="font-variant: small-caps">ExperimentalSetup</font> information unit is modeled. 
					The reference paper is <a href="https://arxiv.org/abs/1409.0473" target="_blank">Neural Machine Translation by Jointly Learning to Align and Translate</a>.
                </p>
                <div class="code-txt">
                <pre>
{
  "has" : {
    "Experimental setup" : {
      "train" : {
        "two types of models" : {
          "first one" : "RNN Encoder–Decoder (RNNencdec, Cho et al., 2014a)",
          "other" : "RNNsearch",
          "from sentence" : "We train two types of models. The first one is an RNN Encoder–Decoder (RNNencdec, Cho et al., 2014a), and the other is the proposed model, to which we refer as RNNsearch."          
        },
        "each model twice" : {
          "with" : [
            "sentences of length up to 30 words",
            "sentences of length up to 50 word"
          ],
          "from sentence" : "We train each model twice: first with the sentences of length up to 30 words (RNNencdec-30, RNNsearch-30) and then with the sentences of length up to 50 word (RNNencdec-50, RNNsearch-50)."
        },
        "minibatch stochastic gradient descent (SGD) algorithm" : {
          "with" : "Adadelta",
          "update direction" : "minibatch of 80 sentences",
          "from sentence" : "We use a minibatch stochastic gradient descent (SGD) algorithm together with Adadelta (Zeiler, 2012) to train each model."
        }
      },
      "RNNencdec" : {
        "encoder and decoder" : {
          "hidden units" : "1000"
        },
        "from sentence" : "The encoder and decoder of the RNNencdec have 1000 hidden units each."
      },
      "RNNsearch" : {
        "encoder" : {
          "consists of" : {
            "forward and backward recurrent neural networks (RNN)" : {
              "hidden units" : "1000"
            }
          }
        },
        "decoder" : {
          "hidden units" : "1000"
        },
        "multilayer network" : {
          "with" : {
            "single maxout (Goodfellow et al., 2013) hidden layer" : {
              "to compute" : "conditional probability of each target word"
            }
          }
        },
        "from sentence" : "The encoder of the RNNsearch consists of forward and backward recurrent neural networks (RNN) each having 1000 hidden units. Its decoder has 1000 hidden units. In both cases, we use a multilayer network with a single maxout (Goodfellow et al., 2013) hidden layer to compute the conditional probability of each target word (Pascanu et al., 2014)."
      }
    }
  }
}
				</pre>
                </div>
				
				<p class="lead">From the above annotations, the following 18 triples are obtained. These annotations comprise three levels of nested data overall.</p>
                <div class="code-txt">
                <pre>
(<font style="font-variant: small-caps">Contribution</font>, has, <font style="font-variant: small-caps">ExperimentalSetup</font>)
&nbsp;&nbsp;(<font style="font-variant: small-caps">ExperimentalSetup</font>, train, <font style="font-variant: small-caps">two types of models</font>)
&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">two types of models</font>, first one, <font style="font-variant: small-caps">RNN Encoder–Decoder (RNNencdec, Cho et al., 2014a)</font>)
&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">two types of models</font>, other, <font style="font-variant: small-caps">RNNsearch</font>)
&nbsp;&nbsp;(<font style="font-variant: small-caps">ExperimentalSetup</font>, train, <font style="font-variant: small-caps">each model twice</font>)
&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">each model twice</font>, with, <font style="font-variant: small-caps">sentences of length up to 30 words</font>)
&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">each model twice</font>, with, <font style="font-variant: small-caps">sentences of length up to 50 word</font>)
&nbsp;&nbsp;(<font style="font-variant: small-caps">ExperimentalSetup</font>, train, <font style="font-variant: small-caps">minibatch stochastic gradient descent (SGD) algorithm</font>)
&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">minibatch stochastic gradient descent (SGD) algorithm</font>, with, <font style="font-variant: small-caps">Adadelta</font>)
&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">minibatch stochastic gradient descent (SGD) algorithm</font>, update direction, <font style="font-variant: small-caps">minibatch of 80 sentences</font>)
&nbsp;&nbsp;(<font style="font-variant: small-caps">ExperimentalSetup</font>, RNNencdec, <font style="font-variant: small-caps">encoder and decoder</font>)
&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">encoder and decoder</font>, hidden units, <font style="font-variant: small-caps">1000</font>)	 
&nbsp;&nbsp;(<font style="font-variant: small-caps">ExperimentalSetup</font>, RNNsearch, <font style="font-variant: small-caps">encoder</font>)
&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">encoder</font>, consists of, <font style="font-variant: small-caps">forward and backward recurrent neural networks (RNN)</font>)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">forward and backward recurrent neural networks (RNN)</font>, hidden units, <font style="font-variant: small-caps">1000</font>)
&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">decoder</font>, hidden units, <font style="font-variant: small-caps">1000</font>)
&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">multilayer network</font>, with, <font style="font-variant: small-caps">single maxout (Goodfellow et al., 2013) hidden layer</font>)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">single maxout (Goodfellow et al., 2013) hidden layer</font>, to compute, <font style="font-variant: small-caps">conditional probability of each target word</font>)
				</pre>
                </div>				
				
				<p> </p>				
                <!-- Example 5-->
                <u><h4 class="section-heading">Example #5</h4></u>
                <p class="lead">
                    In this example, the <font style="font-variant: small-caps">Model</font> information unit that essentially reflects the core of the article's contribution is modeled. 
					The reference paper is <a href="http://papers.nips.cc/paper/5550-convolutional-neural-network-architectures-for-mat" target="_blank">Convolutional Neural Network Architectures for Matching Natural Language Sentences</a>.
                </p>
                <div class="code-txt" style="max-height: 380vh">
                <pre>
{
  "has" : {
    "Model" : {
      "is" : ["deep neural network models", {"from sentence" : "Towards this end, we propose deep neural network models, which adapt the convolutional strategy (proven successful on image [11] and speech [1]) to natural language."}],
      "adapt" : ["convolutional strategy", {"from sentence" : "Towards this end, we propose deep neural network models, which adapt the convolutional strategy (proven successful on image [11] and speech [1]) to natural language."}],
      "naturally host" : {
        "hierarchical composition for sentences" : {
          "with" : {
            "same convolutional architecture" : {
              "To further explore" : "relation between representing sentences and matching them"
            }
          },
          "from sentence" : "To further explore the relation between representing sentences and matching them, we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple-to-comprehensive fusion of matching patterns with the same convolutional architecture."            
        },
        "simple-to-comprehensive fusion of matching patterns" : {
          "with" : {
            "same convolutional architecture" : {
              "To further explore" : "relation between representing sentences and matching them"
            }
          },
          "from sentence" : "To further explore the relation between representing sentences and matching them, we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple-to-comprehensive fusion of matching patterns with the same convolutional architecture."            
        }          
      }
    }
  }
}
				</pre>
                </div>

				<p class="lead">From the above annotations, the following nine triples are obtained. These annotations comprise three levels of nested data overall.</p>
                <div class="code-txt">
                <pre>
								
(<font style="font-variant: small-caps">Contribution</font>, has, <font style="font-variant: small-caps">Model</font>)
&nbsp;&nbsp;(<font style="font-variant: small-caps">Model</font>, is, <font style="font-variant: small-caps">deep neural network models</font>)
&nbsp;&nbsp;(<font style="font-variant: small-caps">Model</font>, adapt, <font style="font-variant: small-caps">convolutional strategy</font>)
&nbsp;&nbsp;(<font style="font-variant: small-caps">Model</font>, naturally host, <font style="font-variant: small-caps">hierarchical composition for sentences</font>)
&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">hierarchical composition for sentences</font>, with, <font style="font-variant: small-caps">same convolutional architecture</font>)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">same convolutional architecture</font>, To further explore, <font style="font-variant: small-caps">relation between representing sentences and matching them</font>)
&nbsp;&nbsp;(<font style="font-variant: small-caps">Model</font>, naturally host, <font style="font-variant: small-caps">simple-to-comprehensive fusion of matching patterns</font>)
&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">simple-to-comprehensive fusion of matching patterns</font>, with, <font style="font-variant: small-caps">same convolutional architecture</font>)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(<font style="font-variant: small-caps">same convolutional architecture</font>, To further explore, <font style="font-variant: small-caps">relation between representing sentences and matching them</font>)

				</pre>
                </div>				
				
				<p> </p>								
				

            <div class="col-lg-5 mr-auto">
                <!--<img class="img-fluid" src="img/task.png" alt="">-->
            </div>


        </div>
    </div>
    <!-- /.container -->
</section>

</body>


</html>


